{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c07c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community langchain-core \\\n",
    "    langchain-google-genai google-generativeai duckduckgo-search gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098f83cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import gradio as gr\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")  # ‚Üê  Access from environment variable\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Load Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e99c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Summarizer\n",
    "def get_summary(text):\n",
    "    prompt = f\"Summarize the following text:\\n{text}\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# 2. Math Solver\n",
    "def solve_math(query):\n",
    "    prompt = f\"Solve this math problem:\\n{query}\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# 3. Web Search (DuckDuckGo)\n",
    "duck_tool = DuckDuckGoSearchRun()\n",
    "def search_web(query):\n",
    "    return duck_tool.run(query)\n",
    "\n",
    "# 4. Translator\n",
    "def get_translation(text, lang=\"Hindi\"):\n",
    "    prompt = f\"Translate this text to {lang}:\\n{text}\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# 5. Code Explainer\n",
    "def code_explainer(code):\n",
    "    prompt = f\"Explain what this code does:\\n{code}\"\n",
    "    return llm.invoke(prompt).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55eaf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 45\n",
      "}\n",
      "].\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 2235, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_23700\\1737110149.py\", line 9, in solve_math\n",
      "    return llm.invoke(prompt).content\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 383, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1006, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 825, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1072, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 961, in _generate\n",
      "    response: GenerateContentResponse = _chat_with_retry(\n",
      "                                        ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 196, in _chat_with_retry\n",
      "    return _chat_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py\", line 338, in wrapped_f\n",
      "    return copy(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py\", line 477, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py\", line 378, in iter\n",
      "    result = action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py\", line 420, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py\", line 187, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tenacity\\__init__.py\", line 480, in __call__\n",
      "    result = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 194, in _chat_with_retry\n",
      "    raise e\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_google_genai\\chat_models.py\", line 178, in _chat_with_retry\n",
      "    return generation_method(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py\", line 835, in generate_content\n",
      "    response = rpc(\n",
      "               ^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py\", line 131, in __call__\n",
      "    return wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 294, in retry_wrapped_func\n",
      "    return retry_target(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 156, in retry_target\n",
      "    next_sleep = _retry_error_helper(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py\", line 214, in _retry_error_helper\n",
      "    raise final_exc from source_exc\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py\", line 147, in retry_target\n",
      "    result = target()\n",
      "             ^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\timeout.py\", line 130, in func_with_timeout\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py\", line 78, in error_remapped_callable\n",
      "    raise exceptions.from_grpc_error(exc) from exc\n",
      "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(title=\"LangChain Multi-Function AI\") as demo:\n",
    "    gr.Markdown(\"## ü§ñ LangChain AI Agent with 5 Functions\")\n",
    "\n",
    "    with gr.Tab(\"1. Summarizer\"):\n",
    "        text_input = gr.Textbox(label=\"Enter text to summarize\")\n",
    "        text_output = gr.Textbox()\n",
    "        btn = gr.Button(\"Summarize\")\n",
    "        btn.click(fn=get_summary, inputs=text_input, outputs=text_output)\n",
    "\n",
    "    with gr.Tab(\"2. Math Solver\"):\n",
    "        math_input = gr.Textbox(label=\"Enter math problem\")\n",
    "        math_output = gr.Textbox()\n",
    "        btn2 = gr.Button(\"Solve\")\n",
    "        btn2.click(fn=solve_math, inputs=math_input, outputs=math_output)\n",
    "\n",
    "    with gr.Tab(\"3. Web Search\"):\n",
    "        web_input = gr.Textbox(label=\"Search Query\")\n",
    "        web_output = gr.Textbox()\n",
    "        btn3 = gr.Button(\"Search\")\n",
    "        btn3.click(fn=search_web, inputs=web_input, outputs=web_output)\n",
    "\n",
    "    with gr.Tab(\"4. Translator\"):\n",
    "        trans_input = gr.Textbox(label=\"Text to translate\")\n",
    "        lang_input = gr.Textbox(value=\"Hindi\", label=\"Target Language\")\n",
    "        trans_output = gr.Textbox()\n",
    "        btn4 = gr.Button(\"Translate\")\n",
    "        btn4.click(fn=get_translation, inputs=[trans_input, lang_input], outputs=trans_output)\n",
    "\n",
    "    with gr.Tab(\"5. Code Explainer\"):\n",
    "        code_input = gr.Textbox(label=\"Paste your code\")\n",
    "        code_output = gr.Textbox()\n",
    "        btn5 = gr.Button(\"Explain Code\")\n",
    "        btn5.click(fn=code_explainer, inputs=code_input, outputs=code_output)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc332fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
